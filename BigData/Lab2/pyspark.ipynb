{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fe26f5e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f075fc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Csv VS Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa146f5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Connect to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c561be5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/10 17:21:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .appName(\"pyspark\")\\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .config(\"spark.executor.memory\", \"1g\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e985d31",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "book_schema = StructType([\n",
    "    StructField(\"Id\", IntegerType()),\n",
    "    StructField(\"Name\", StringType()),\n",
    "    StructField(\"PagesNumber\", IntegerType()),\n",
    "    StructField(\"Authors\", StringType()),\n",
    "    StructField(\"Publisher\", StringType()),\n",
    "    StructField(\"Language\", StringType()),\n",
    "    StructField(\"Rating\", DoubleType()),\n",
    "    StructField(\"RatingDist1\", IntegerType()),\n",
    "    StructField(\"RatingDist2\", IntegerType()),\n",
    "    StructField(\"RatingDist3\", IntegerType()),\n",
    "    StructField(\"RatingDist4\", IntegerType()),\n",
    "    StructField(\"RatingDist5\", IntegerType()),\n",
    "    StructField(\"RatingDistTotal\", IntegerType()),\n",
    "    StructField(\"CountsOfReview\", IntegerType()),\n",
    "    StructField(\"PublishDay\", IntegerType()),\n",
    "    StructField(\"PublishMonth\", IntegerType()),\n",
    "    StructField(\"PublishYear\", IntegerType()),\n",
    "    StructField(\"ISBN\", StringType())\n",
    "])\n",
    "\n",
    "user_schema = StructType([\n",
    "    StructField(\"ID\", IntegerType()),\n",
    "    StructField(\"Name\", StringType()),\n",
    "    StructField(\"Rating\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c79bcf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f6b752f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def process_book_csv(df: pd.DataFrame):\n",
    "    if 'PagesNumber' not in df.columns:\n",
    "        df = df.rename(columns={\"pagesNumber\": \"PagesNumber\"})\n",
    "\n",
    "    df['RatingDist1'] = df['RatingDist1'].str[2:].astype(int)\n",
    "    df['RatingDist2'] = df['RatingDist2'].str[2:].astype(int)\n",
    "    df['RatingDist3'] = df['RatingDist3'].str[2:].astype(int)\n",
    "    df['RatingDist4'] = df['RatingDist4'].str[2:].astype(int)\n",
    "    df['RatingDist5'] = df['RatingDist5'].str[2:].astype(int)\n",
    "    df['RatingDistTotal'] = df['RatingDistTotal'].str[6:].astype(int)\n",
    "\n",
    "    df = df.loc[:, [x.name for x in book_schema.fields]]\n",
    "    return spark.createDataFrame(df, schema=book_schema)\n",
    "\n",
    "def process_user_csv(df: pd.DataFrame):\n",
    "    df = df.loc[:, [x.name for x in user_schema.fields]]\n",
    "    return spark.createDataFrame(df, schema=user_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "982d85f0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading csv: book1-100k.csv\n",
      "Reading csv: book1000k-1100k.csv\n",
      "Reading csv: book100k-200k.csv\n",
      "Reading csv: book1100k-1200k.csv\n",
      "Reading csv: book1200k-1300k.csv\n",
      "Reading csv: book1300k-1400k.csv\n",
      "Reading csv: book1400k-1500k.csv\n",
      "Reading csv: book1500k-1600k.csv\n",
      "Reading csv: book1600k-1700k.csv\n",
      "Reading csv: book1700k-1800k.csv\n",
      "Reading csv: book1800k-1900k.csv\n",
      "Reading csv: book1900k-2000k.csv\n",
      "Reading csv: book2000k-3000k.csv\n",
      "Reading csv: book200k-300k.csv\n",
      "Reading csv: book3000k-4000k.csv\n",
      "Reading csv: book300k-400k.csv\n",
      "Reading csv: book4000k-5000k.csv\n",
      "Reading csv: book400k-500k.csv\n",
      "Reading csv: book500k-600k.csv\n",
      "Reading csv: book600k-700k.csv\n",
      "Reading csv: book700k-800k.csv\n",
      "Reading csv: book800k-900k.csv\n",
      "Reading csv: book900k-1000k.csv\n",
      "Reading csv: user_rating_0_to_1000.csv\n",
      "Reading csv: user_rating_1000_to_2000.csv\n",
      "Reading csv: user_rating_2000_to_3000.csv\n",
      "Reading csv: user_rating_3000_to_4000.csv\n",
      "Reading csv: user_rating_4000_to_5000.csv\n",
      "Reading csv: user_rating_5000_to_6000.csv\n",
      "Reading csv: user_rating_6000_to_11000.csv\n"
     ]
    }
   ],
   "source": [
    "book_df = spark.createDataFrame([], schema=book_schema)\n",
    "user_df = spark.createDataFrame([], schema=user_schema)\n",
    "\n",
    "for path, _, files in os.walk('./data/'):\n",
    "    for filename in sorted(files):\n",
    "        csv_path = os.path.join(path, filename)\n",
    "        print(\"Reading csv: {}\".format(filename))\n",
    "        csv_df = pd.read_csv(csv_path)\n",
    "\n",
    "        if \"book\" in filename:\n",
    "            book_df = book_df.union(process_book_csv(csv_df))\n",
    "        elif \"user\" in filename:\n",
    "            user_df = user_df.union(process_user_csv(csv_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3864807e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/10 17:23:54 WARN TaskSetManager: Stage 1 contains a task of very large size (1820 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 1:=============================>                             (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------+--------------------+---------------+--------+------------------+-----------+-----------+-----------+-----------+-----------+---------------+--------------+----------+------------+-----------+----------+\n",
      "| Id|                Name|PagesNumber|             Authors|      Publisher|Language|            Rating|RatingDist1|RatingDist2|RatingDist3|RatingDist4|RatingDist5|RatingDistTotal|CountsOfReview|PublishDay|PublishMonth|PublishYear|      ISBN|\n",
      "+---+--------------------+-----------+--------------------+---------------+--------+------------------+-----------+-----------+-----------+-----------+-----------+---------------+--------------+----------+------------+-----------+----------+\n",
      "|  1|Harry Potter and ...|        652|        J.K. Rowling|Scholastic Inc.|     eng|              4.57|       9896|      25317|     159960|     556485|    1546466|        2298124|         28062|         9|          16|       2006|       NaN|\n",
      "|  2|Harry Potter and ...|        870|        J.K. Rowling|Scholastic Inc.|     eng|               4.5|      12455|      37005|     211781|     604283|    1493113|        2358637|         29770|         9|           1|       2004|0439358078|\n",
      "|  3|Harry Potter and ...|        309|        J.K. Rowling| Scholastic Inc|     eng|              4.47|     108202|     130310|     567458|    1513191|    4268227|        6587388|         75911|        11|           1|       2003|       NaN|\n",
      "|  4|Harry Potter and ...|        352|        J.K. Rowling|     Scholastic|     eng|              4.42|      11896|      49353|     288821|     706082|    1504505|        2560657|           244|        11|           1|       2003|0439554896|\n",
      "|  5|Harry Potter and ...|        435|        J.K. Rowling|Scholastic Inc.|     eng|              4.57|      10128|      24849|     194848|     630534|    1749958|        2610317|         37093|         5|           1|       2004|043965548X|\n",
      "|  6|Harry Potter and ...|        734|        J.K. Rowling|     Scholastic|     eng|4.5600000000000005|       9419|      24282|     178419|     606800|    1612165|        2431085|         31978|         9|          28|       2002|       NaN|\n",
      "|  8|Harry Potter Boxe...|       2690|        J.K. Rowling|     Scholastic|     eng|              4.78|        402|        283|       1201|       4650|      37432|          43968|           166|         9|          13|       2004|0439682584|\n",
      "|  9|Unauthorized Harr...|        152|W. Frederick Zimm...|   Nimble Books|   en-US|              3.79|          0|          5|          6|          7|         10|             28|             1|         4|          26|       2005|0976540606|\n",
      "| 10|Harry Potter Coll...|       3342|        J.K. Rowling|     Scholastic|     eng|              4.73|        257|        218|       1074|       4358|      24406|          30313|           809|         9|          12|       2005|0439827604|\n",
      "| 12|The Ultimate Hitc...|        815|       Douglas Adams| Gramercy Books|     eng|              4.37|       3443|       7613|      30030|      75683|     157499|         274268|           255|        11|           1|       2005|0517226952|\n",
      "+---+--------------------+-----------+--------------------+---------------+--------+------------------+-----------+-----------+-----------+-----------+-----------+---------------+--------------+----------+------------+-----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "book_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafbd1a0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Write data to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cbe2dc7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/10 17:24:02 WARN TaskSetManager: Stage 2 contains a task of very large size (1820 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/04/10 17:24:11 WARN TaskSetManager: Stage 3 contains a task of very large size (1820 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/04/10 17:24:27 WARN TaskSetManager: Stage 4 contains a task of very large size (1241 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "book_df.write.option(\"header\", True)\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .csv(\"hdfs://hadoop-namenode:9000/books/data.csv\")\n",
    "\n",
    "book_df.write.option(\"header\", True)\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .parquet(\"hdfs://hadoop-namenode:9000/books/data.parquet\")\n",
    "\n",
    "user_df.write.option(\"header\", True)\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .parquet(\"hdfs://hadoop-namenode:9000/users/data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7727f8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Compare read speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8a1e53d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Csv read complete in 0.7820 seconds\n",
      "Parquet read complete in 0.0904 seconds\n"
     ]
    }
   ],
   "source": [
    "csv_start = time.perf_counter()\n",
    "csv_load = spark.read.csv(\"hdfs://hadoop-namenode:9000/books/data.csv\")\n",
    "csv_finish = time.perf_counter()\n",
    "\n",
    "parquet_start = time.perf_counter()\n",
    "parquet_load = spark.read.parquet(\"hdfs://hadoop-namenode:9000/books/data.parquet\")\n",
    "parquet_finish = time.perf_counter()\n",
    "\n",
    "print(f\"Csv read complete in {csv_finish - csv_start:0.4f} seconds\")\n",
    "print(f\"Parquet read complete in {parquet_finish - parquet_start:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b378beee",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447cede8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## a) Top 10 books by reviews count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16e3bf09",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/10 17:24:42 WARN TaskSetManager: Stage 8 contains a task of very large size (1820 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 8:======================================================>  (91 + 4) / 96]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|                Name|CountsOfReview|\n",
      "+--------------------+--------------+\n",
      "|The Hunger Games ...|        154447|\n",
      "|Twilight (Twiligh...|         94850|\n",
      "|      The Book Thief|         87685|\n",
      "|            The Help|         76040|\n",
      "|Harry Potter and ...|         75911|\n",
      "|The Giver (The Gi...|         57034|\n",
      "| Water for Elephants|         52918|\n",
      "|The Girl with the...|         52225|\n",
      "|Harry Potter and ...|         52088|\n",
      "|The Lightning Thi...|         48630|\n",
      "+--------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "book_df\\\n",
    "    .select(\"Name\", \"CountsOfReview\")\\\n",
    "    .orderBy(desc(\"CountsOfReview\"))\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd9afc1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## b) Top 10 publishers by average pages count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df1a04d1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/10 17:24:46 WARN TaskSetManager: Stage 9 contains a task of very large size (1820 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 10:====================================>                 (135 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|           Publisher|  avg(PagesNumber)|\n",
      "+--------------------+------------------+\n",
      "|Crafty Secrets Pu...|         1807321.6|\n",
      "|    Sacred-texts.com|          500000.0|\n",
      "|Department of Rus...| 322128.5714285714|\n",
      "|Logos Research Sy...|          100000.0|\n",
      "|Encyclopedia Brit...|           32642.0|\n",
      "|Progressive Manag...|        19106.3625|\n",
      "|Still Waters Revi...|10080.142857142857|\n",
      "|P. Shalom Publica...|            8539.0|\n",
      "|Hendrickson Publi...|            6448.0|\n",
      "|            IEEE/EMB|            6000.0|\n",
      "+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "book_df\\\n",
    "    .groupBy(\"Publisher\")\\\n",
    "    .avg(\"PagesNumber\")\\\n",
    "    .orderBy(avg(\"PagesNumber\").desc())\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af7ea37",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## c) Top 10 years by published books count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0c9b305",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/10 17:24:52 WARN TaskSetManager: Stage 11 contains a task of very large size (1820 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 11:======================================================> (94 + 2) / 96]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|PublishYear| count|\n",
      "+-----------+------+\n",
      "|       2007|129507|\n",
      "|       2006|122374|\n",
      "|       2005|117639|\n",
      "|       2004|105733|\n",
      "|       2003|104345|\n",
      "|       2002| 95537|\n",
      "|       2001| 88228|\n",
      "|       2000| 87290|\n",
      "|       2008| 80265|\n",
      "|       1999| 80155|\n",
      "+-----------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "book_df\\\n",
    "    .groupBy(\"PublishYear\")\\\n",
    "    .count()\\\n",
    "    .orderBy(count(\"*\").desc())\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774f0146",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## d) Top 10 books with more than 500 ratings by rating variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd9c7c41",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/10 17:24:56 WARN TaskSetManager: Stage 13 contains a task of very large size (1820 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 13:========================================>               (69 + 4) / 96]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|                Name|          Variance|\n",
      "+--------------------+------------------+\n",
      "|Scientology: The ...|2.8119821805392733|\n",
      "|Scientology: The ...| 2.807017661097852|\n",
      "|Para Entrenar a u...|2.7552864734299516|\n",
      "| To Train Up a Child| 2.754064547677261|\n",
      "|Para Entrenar a u...|2.7539059975520193|\n",
      "|The Bluebook: A U...| 2.522759778597786|\n",
      "|The Bluebook: A U...| 2.522759778597786|\n",
      "|Dianetics: The Mo...| 2.440198032447359|\n",
      "|Dianetics: The Mo...|2.4364041826420357|\n",
      "|Dianetica: La Cie...|2.4347216783216785|\n",
      "+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "book_df\\\n",
    "    .filter(book_df.RatingDistTotal > 500)\\\n",
    "    .withColumn(\"Variance\",\n",
    "       (book_df.RatingDist1 * (1 - book_df.Rating) ** 2 +\n",
    "        book_df.RatingDist2 * (2 - book_df.Rating) ** 2 +\n",
    "        book_df.RatingDist3 * (3 - book_df.Rating) ** 2 +\n",
    "        book_df.RatingDist4 * (4 - book_df.Rating) ** 2 +\n",
    "        book_df.RatingDist5 * (5 - book_df.Rating) ** 2) / book_df.RatingDistTotal)\\\n",
    "    .select(\"Name\", \"Variance\")\\\n",
    "    .orderBy(col(\"Variance\").desc())\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b01a041",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## e) Harry and Potter books statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d589861c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/10 17:25:26 WARN TaskSetManager: Stage 14 contains a task of very large size (1820 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/04/10 17:25:28 WARN TaskSetManager: Stage 15 contains a task of very large size (1820 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/04/10 17:25:30 WARN TaskSetManager: Stage 16 contains a task of very large size (1820 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 16:================================================>       (83 + 4) / 96]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+-----------------+\n",
      "|Title contains|Count|   Average rating|\n",
      "+--------------+-----+-----------------+\n",
      "|         Harry| 1321|3.613557910673732|\n",
      "|        Potter| 1281|3.279008587041375|\n",
      "|  Harry Potter|  405|4.138024691358027|\n",
      "+--------------+-----+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "book_df.filter(book_df.Name.contains(\"Harry\")).select(lit(\"Harry\").alias(\"Title contains\"), count(\"*\").alias(\"Count\"), avg(book_df.Rating).alias(\"Average rating\"))\\\n",
    "    .union(book_df.filter(book_df.Name.contains(\"Potter\")).select(lit(\"Potter\"), count(\"*\"), avg(book_df.Rating)))\\\n",
    "    .union(book_df.filter(book_df.Name.contains(\"Harry\") & book_df.Name.contains(\"Potter\")).select(lit(\"Harry Potter\"), count(\"*\"), avg(book_df.Rating)))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1916ce03",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b910529",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74a5f923",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rating_map = {\n",
    "    \"it was amazing\" : 5,\n",
    "    \"really liked it\" : 4,\n",
    "    \"liked it\" : 3,\n",
    "    \"it was ok\" : 2,\n",
    "    \"did not like it\" : 1\n",
    "}\n",
    "\n",
    "mapping_expr = create_map([lit(x) for x in chain(*rating_map.items())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68b8ce4b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stream_df = spark.readStream\\\n",
    "    .schema(user_schema)\\\n",
    "    .parquet(\"hdfs://hadoop-namenode:9000/users/data.parquet\")\\\n",
    "    .filter(col(\"Rating\") != \"This user doesn't have any rating\")\\\n",
    "    .withColumn(\"RatingValue\", mapping_expr[col('Rating')])\\\n",
    "    .withColumn(\"timestamp\", current_timestamp())\\\n",
    "    .withWatermark(\"timestamp\", \"10 seconds\") \\\n",
    "    .groupBy(\"Name\", window(col(\"timestamp\"), \"15 seconds\"))\\\n",
    "    .agg(avg(\"RatingValue\").alias(\"AverageRating\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b647806d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stream_df.writeStream\\\n",
    "    .format(\"parquet\")\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"path\", \"hdfs://hadoop-namenode:9000/streaming/res/book_ratings.parquet\")\\\n",
    "    .option(\"checkpointLocation\", \"hdfs://hadoop-namenode:9000/streaming/checkpoints/book_ratings\")\\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
