# HW01: Spark

## Блок 1. Standalone Spark
Предыдущий [.docker-compose](docker-compose.yml) с конфигурацией ***1 NN, 1 DN + 1 NM, 1 RM, 1 HS*** был доработан  

В него был добавлен блок Spark ***Master + 2 Workers***, workeraм было выдано по 2 ядра и 2 ГБ ОЗУ каждому  
Jupyter notebook на этот раз был развернут прямо внутри кластера с прокинутой сетью и замаунченной папкой на диск  

Все это позволило крайне просто закинуть данные в докер и сразу же иметь доступ из тетрадки и к хадупу, и к спарку  
DataSpell позволил подключиться к удаленной тетрадке и работать внутри IDE, а не из браузера (что несомененно намного удобнее)

![Spark](https://user-images.githubusercontent.com/48883672/230968919-df666cb4-b088-4330-a12b-ce258f33816b.png)

![Jupyter notebook](https://user-images.githubusercontent.com/48883672/230968936-bd716891-fbfd-402d-bade-687cd78a9ef5.png)

## Блок 2. Работа с данными на Spark

### Расположить данные в HDFS

Весь код, описанный ниже можно найти в [тетрадке](pyspark.ipynb)

Подключение к sparkу осуществлялось через фреймворк PySpark, используя класс ***SparkSession***  
Выделил 1 GB RAM на каждом Workere, что хватает с запасом для текущего датасета

Создал схемы для маппинга книг и оценок в spark-схемы

Далее прошелся по всем файлам в директории, соединяя их в рамках ***Spark*** DataFramов  
Для книг пришлось предобработать данные:
* Колонка ***PagesNumber*** была в разных регистрах в разных *.csv* файлах
* Распределение оценок было в ~~кринжовом~~ формате, пришлось обрезать начало и сделать их числами

Записал оба DataFrama в **HDFS** в формате *.parquet*, книги записал еще в *.csv*, чтобы замерить разницу в скорости чтения и размере

А разница-то существенная, что по размеру, что скорости:

| Format   | Before repl | After repl | Read time |
|----------|-------------|------------|-----------|
| .parquet | 135.9 MB    | 407.7 MB   | 0.0904 s  |
| .csv     | 246.4 MB    | 739.1 MB   | 0.7820 s  |

Не буду в дальнейшем юзать *.csv* для хранения больших данных :)

### Анализ данных

Выполнил все SQL запросы из задания на полученном Spark DataFrame

Как интересный инсайт выбрал анализ книжек по Гарри Поттеру

| Title contains | Count | Average rating    |
|----------------|-------|-------------------|
| Harry          | 1321  | 3.613557910673732 |
| Potter         | 1281  | 3.279008587041375 |
| Harry Potter   | 405   | 4.138024691358027 |

Как видно наличие одновременно и имени, и фамилии в названии книги существенно повышает её рейтинг

## Блок 3. Работа с данными на Spark

Для последнего задания использовал *.parquet* файл с юзерами из предыдущего задания

Пришлось его немного предобработать:
* Убрал все строчки пользователей, не оставивших ни одного отзыва
* Замапил текстовые оценки в числовые значения
* Добавил таймстемп для каждой строки

Последний пункт был специально сделан для использования в качестве watermarkи  
Записал агрегированные результаты обратно в **HDFS** в формате *.parquet*
